{
    "metadata": {
        "video_id": "qgLJUG71SVM",
        "title": "EDCON Keynote Presentation with Vitalik Buterin & the Ethereum Foundation | EDCON 2019",
        "link": "https://www.youtube.com/watch?v=qgLJUG71SVM",
        "duration": "1:53:29",
        "channel_name": "Crypto Finder"
    },
    "transcript": {
        "0": "I dunno Constantinople is done for ",
        "1": "plasma Bluebell to engage no fork galavant coming in waves fork your boy Carlos is from new plasma state channels layers one layer two layer four eight sixteen binary Bitcoin blockchain trust human coordination you know the drill morkul plasma chains but what privacy scaling it up snap step constant product Oracle's tap tap plasma chains flat plat seeking stars coming solo clap clap privacy scaling it up snap snap how's your products over cozy solo cup detects not just a free compiler change of op codes we're here to change the ",
        "2": "rails afraid all her fall notes he wasn't as hard but what I do know is with abstract Akash will chase a hoe can go if you point oh yeah he's to point now yeah you put out you put out we'll go octahedral we must coordinate but we had no Cathedral the one subordinate every team in every project we could get Bart Superchick biologic we work like a bazaar yeah taking all in the first phase spork all of these coming in waves spork to buzz it's coming your way it's port jump or Carlos's from you sure get to point out yeah yeah you point out yeah difficulty Ice Age meltdown clear what will your 3 9 27 security this is what happens when ",
        "3": "you code a lot wideness failure what are we going to do welcome to the stage hello hello everyone happy to meet you again and happy to meet so many new friends come coming to aircon this year the assyrian ",
        "4": "community a great community and it is exciting to see it grow so much with every year first and most importantly I want to thank everyone who helped us to make the conference happen although the market is a little cold but all sponsors are warm I want to thank them they they have given us so much supporters to help us to make this event happen we try hard to make the event as a sensible for people as we can we offer many developers and the student discounts so we rely on support of our sponsors thank ",
        "5": "you very much this year we have even more community supporters and the medias than before from our word word now I want to thank all the speakers most of them fly very far to come to see me this time we have almost all the call the researchers and many developers of cerium - we recycling the project to make a cerium Android of times more efficient and us keyboard skeeball means proof of still can - Artie we also have many researchers and developers of plasma and many Assyrian applications both for-profit and nonprofit the hard work put in by all of these people both old and new is what ",
        "6": "continues to make the Syrian community great let me introduce at home for new friends this is our first article appearance in [Music] 2017 and this is this was offer a head count in Toronto last year and now this this is an addict on Sydney I hope you will like a head count I had annually early in April on May 14 a static home we are thinking where we should go if you have a good suggestion please email to us we are looking for location they are easy to travel and get around and that can hold more than 1,000 ",
        "7": "people and easy to get visas aside from any current events we have also organized a large e serial Misha like this one inch engine and this one in Bangkok in February last year and this one in Beijing last year and this one in Hong Kong hope to see you in more cities to help you enjoy at home I need to introduce some details about the conference this year our presentations will be happening in the main conference and the conference room and the conference room and there will be parties and happy hours events organized by community organizations you can look ",
        "8": "at the aircon website to see what's the events are ok here is a map of the conference venue the map is also available at the front desk if you need it you can go there to check we hope that aircon can be and opportunities for people from many countries and communities and many backgrounds including technology business and the research to come together this year we have people coming from at least 15 countries we try to be flexible and meet the needs of different speakers and the Gil as many people as possible chance to speak we believe in treating people fairly and try to choose speeches to bring as much value as possible from all attendees do not just ",
        "9": "be about presentations but also give people an opportunity to meet and talk to each other and hopefully find the opportunities to cooperate in the future let me introduce a starling time in fact we are technology company but we like Honda good at making community events aircon is our biggest and best conference series we are also developing an Assyrian wallet called panda expand axes for you serum or wallet including the ability to create and the importer accounts and make payments supply support for either and ICRC 20 tokens add a browser and tokens web and many ",
        "10": "more feature hope you try and I look forward to cooperating with you this and carmaker is offering an hour job of ten thousand a diet that you can clean to claim your share you scan the QR code to install panda eggs fund the aircon depth on the application page click the registered address icon and show the QR code to that staff as registrant desk then you can use dye on excited to buy any alcohol souvenirs we also have unit hands a global serum community platform anyone is welcome to publish their content on the site and we hope to see more people start using AIDS ",
        "11": "we can also help you build a technology community community in China or Asia we can help you with PR service in China and that we can organized online or offline international blocking events I hope that you can enjoy the conference in the next 2-3 days and I hope to see you again next time finally you won't contaminate can find me here thank you ok let's start to the conference thank you [Applause] what's up everyone welcome thank you for all waking up so bright and early I'm so I'm Ben I'm going to be introducing some of the like leather luxury love lovely ",
        "12": "speakers this morning before we do that I know that Pangaea has put an incredible amount of work and time and energy into this project so I think we should give her another round of applause seriously thank you in any case our first speaker probably needs no introduction in this room but he's going to be talking about the future of youth 2.0 and some research so let's please welcome to the stage but Alec Ritter and good morning raise your hands if you have a good morning raise your hands if you haven't this the side of the question always gets no answer is for some reason it's strange so today ",
        "13": "I'm going to talk about one of the technical developments that's not going to become relevant to aetherium 2.0 immediately it's a slightly later stage three things so possibly face - shortly after phase two I've been calling it part of phase three which is the introduction of a CBC Casper into the etherium 2.0 proof of stake blockchain so first thing I should start off by introducing what CBC Casper is right so CBC consensus is this a new school of a safe under asynchrony Byzantine fault tolerant consensus meaning the kind that works originally invented by Vlad's ampere and the main theme of CBC Casper is that you have basically a proof of stake chain so you have validators and validators keep on making blocks and when they make blocks they make a block on top of the chain that they consider to be the best chain and they ",
        "14": "might pick this with what Vlad calls an estimator I call a fork choice rule and fork choice rules are pretty common so for example the longest chain rule in Bitcoin or in aetherium is a fork choice rule except here there is a twist which is that unlike the longest chain rule we are nomads any given any particular chain there is always the possibility some longer chain exists here there exists this possibility of a kind of locking mechanism where once enough validators build on a particular chain for enough rounds then according to the rules of this fortress rule its that chain becomes locked in it's like not possible to switch to voting or building on another chain without a huge portion of validators like basically committing provable violations which means they get their deposit slashed so in CBC Kaspar finality and the concept of when you know that a block is not going to be reverted is client-side so what this means is that you don't ",
        "15": "need to choose hard thresholds in protocol like the number of 2/3 that you might see in bft protocols more more commonly does not appear in CBC right but you can choose for yourself a threshold of 2/3 but you can choose for yourself a threshold of 0.8 or 0.55 and every user personally has the ability to matures for themselves their own trade off between aliveness and safety so LMT ghost it's this fairly important built building block of at least the the version of a CBC Casper that I'm going to be representing here right so what is lmg ghost so it stands for latest message driven greedy heaviest observed subtree um so it's an adaptation of the ghost protocol created by Jonathan sample in skin if you have Zohar back in 2013 to a proof of state context and the ",
        "16": "way the ghost protocol for proof of work worked is basically that instead of looking at the longest chain you do this kind of block-by-block procedure where every time you reach a block that has two children you pick the child that has more blocks in total supporting it even if those blocks are not part of the same chain now in latest message driven ghosts which is this adaptation to proof of stake we don't look at all of the blocks that have been made because unlike in proof of work we're making ten times more blocks requires ten times more computing effort in proof of stakes you can sign as as much as you want so there's not much more points in counting and so we look only at the most recent messages of any node as evidence of the most recent opinion of what that particular validator is favoring right so now if we can look at this through our diagram so let's assume a kind of simple model ",
        "17": "where the only thing that happens is blocks you don't have a gestation so you don't have mass you don't have any more complicated messages you just have blocks so you might imagine time being and ifs plus split up into epochs during each epoch every validator has a chance to create a block so here we're assuming five validators where each valid validator makes a block once over here on the left so a B C D and E are the validators and then each validator makes a block on the right and so if from your point of view as a client your view so the blocks that you have seen and with this then you figure out what the latest map blocks are created by each validator in this case from a it's going to be this one from B it's gonna be this one from C it'll be this one from me it will be this one and from D it's still this one because we haven't heard a block from D in this epoch right so this is so this is what latest message driven means right so now what does ghost mean so the ",
        "18": "idea is that we're going to focus on these five latest messages and we're going to assign a score to each block where the score of the block basically is the total number of latest messages that are supporting that block so this block only has one block supporting it which is that block itself walk with me has a score of 1 this block has a score of 1 this block is supported by itself and by this block so it has a score of 2 this block has a score of 4 because it's supported by this this this and this now it's not 5 because this block itself is not a latest block and so it doesn't count this is also 4 now here you have one and then this finally goes up to 5 now you might notice that scores go higher the older a block is so we're not going to just pick the block with the highest score right the idea here is that we're going to start with the with the Genesis block so start over here and then if ",
        "19": "there's one child we hop over to the child and then if you have a choice between two or more blocks then you pick the side that has a higher score so over here you pick bottom then you go to the right and then over here you pick middle cos middle has a score of two and then you go forward and this ends up being your head now you might ask well why do we what do we need why do we care about using LMG ghosts right so for example just a simple longest chain rule would also pick this block in this particular case that's the winner because this block is the unique block that has a chain of length 6 and so even on the longest chain rule it wins as well so why not just use the longest chain rule if it is simpler so the reason why we use LMG ghosts is that there's two parts to it one of them is a kind of common property of ghosts in general which basically is that longest chain rules cannot take into account information from frequent parallel mm attestations ",
        "20": "so what does this mean right so what this means is that in the currents of theory M chain you have a block coming once in every 14 seconds and an and so on average something like over 91% of all blocks make it as part of the main chain and so we can kind of not care about what those blocks that are not part of the main chain say because they don't really affect things that much but now imagine we wanted to have a chain that comes to consensus faster and so instead of one block every 14 seconds we had 14 blocks everyone second now if you do something like that then almost all of those blocks are not going to end up being part of the same chain right just because of network latency if I publish a block by the time that block reaches the entire network there will be a hundred other blocks that other people will have published and so the kind of depth of the chain the the length of the longest that part ",
        "21": "of it will be maybe one percent of the total number of blocks so the problem with this is that if you have an attacker then an attacker just running on their own server could more easily just create a chain which is much longer because they can perfectly coordinate with themselves they don't have any network latency so but in reality though even if you have an attacker that can make a longer chains but there's plenty of information that says that the chain the rest of the network is working on has more support right you can just look at a day you can see that there's like a hundred times more people building on the real chain then there are people building on the attached chain and so the question is well how do we have a formally defined fork choice rule that actually takes this information into account and LMT ghost is the answer right if two blocks appear in parallel they still both count as support for whatever their common ancestor is another interesting property of lmg ghost and this is very important to CBC ",
        "22": "finality specifically and specifically importance to ghost in a proof state context is that it's a minority can never beat the majority regardless of how many messages the minority sends so in this particular case here right we have five validators once again and you might notice that four of them are kind of peacefully happily coordinating on building up one chain I mean you see C and D make up make a little bit of a mistake but otherwise it's just going on fine and then you have B off in a corner possibly disconnected from the network possibly trying to attack it now imagine B makes 500 blocks if you use the longest chain rule B would win if you use proof-of-work style ghost B would also win if you use lmd ghost B would lose because all of those blocks come from B and we only count the latest message anyway so when you make a new message it ",
        "23": "over writes everything that you said before so from these a two property is what we get is Eve or choice rule which is both kind of very friendly to eath 2.0 is a goal of trying to coordinate on blocks very quickly and at the same time something that looks like it might actually kind of finalize in the same sense something like Casper FFG finalizes so let's some take a look at this a view that I showed you in this slide more closely right so let's assume the thing at the top is your global view so you are happily running in aetherium node and you're happy etherium node has received these blocks from the network so that's your global view from your global view you can infer things about the local views of each of the validators so for example one thing you might notice here is that a made-up made their most recent block over here and so ",
        "24": "if a made the most recent block over here that means that at that recent point in time a was building I'm a was aware of these four blocks and sand at that particular point in time a consider these four blocks to be the latest messages if you look at C's view C build a block over here which means C was aware of these four which means you know that C that C sees that these four these four latest messages are kind of pushing the fortress rule in this direction um you know this is the same thing about DS of you and you know the same thing about about easy view right so each of the four validators except for b c is the chain and sees that everyone else is also building on the same chain so from this information we can make a claim right and the claim basically is that unless at least two validators make blocks that violate the protocol rules so make blocks that where you can just ",
        "25": "take that block put it together both a block the validator made before and you get a proof that the validator violated the protocol then the chain will the basically unless at least two to validators break the rules no validator except B has the ability to make a block on - on top of the top chain and so the only thing that can keep growing is the bottom chain and so we say the bottom chain or specifically a is finalized with a quote safety margin of two right so how do we justify this well first of all it's a rule of CBC Gasper that for a block to be valid that block must be that must be the the blocks parents must be the result of executing the fork surest rule given all of the messages that that chain knows about and so here well can Eve build a ",
        "26": "block over here right so first of all for a to do this then a would need to know a would build a block over here but then no one else had buildable built a block yet and so well a would be clearly violating the rules because a moved from a chain that has more support to a chain that has less support can see illegally moved to the top chain well no because he built a block on this chain that's supported by for our validators and this chain has one validator so see can't legally move over Cinzia legally we've over no can illegally move over no and it turns out that you I that you need to eval any two of these validators to break the rules and illegally move over to the top chain for everyone else to be able to follow and for the top chain to win so this is why a is finalized with a safety margin of two now you might notice here that this concept of safety is completely subjective right all that we have here is validators fought following a fortress rule we just have ",
        "27": "validators following a version of ghost that gets that's been adapted to prove of stake and that's been adapted to this context of a and of bounded validator set and out of this you have this concept of finality even though you do not have an explicit two-thirds threshold you do not have in explicit concept of preparing and committing you don't have any of these and if explicit ideas that exist in either Casper FFG or more traditional visiting fault-tolerant consensus algorithms more generally right so this seems like what we've basically managed to do is we've managed to build this concept of finality which previously was a more kind of the domain of these BFC algorithms on top of a proof of sneek algorithm that looks very chain based so it looks very similar to just some of the older proof of stake algorithms that people have been doing for the last five years except by switching from longest chain to LMZ ghost you just kind of ",
        "28": "unlocked the ability to get this finality property right so basically what this says is that a is finalized unless to validators unless 40% of the validator said is willing to burn all of their money to cause the again scientists shift over so more generally in the context of LMZ ghosts there exists a heuristic called the two round quick Oracle which says if there exists a 1/2 plus P sized subset of validators that all support the same chain with two consecutive messages so two rounds of messaging from 1/2 plus P then the earliest of the block signs in those two rounds is finalized with safety margin P now if you want to solve for kind of maximizing safety and maximizing aliveness then you might want to set P to be 1/4 and so if 3/4 of the validators support the same chain with two rounds then you get a safety margin ",
        "29": "of 1/4 so the two round queeg Oracle is sufficient but it's not necessary for finality so there's also the three round quick Oracle there's the the n around quick Oracle there's potentially other kinds of Oracle's so there's this extremely large space of heuristics that you could potentially use to and if compute whether or not finality is happening locally so cool things about CBC Casper right so why is this potentially interesting so first of all different clients can privately choose different required safety margins at which they show a block is finalized and especially if a client believes that some subset of nodes are permanently offline it can privately use that information to verify finality so for example let's suppose that seventy and two percent of aetherium staking is done in Antarctica ",
        "30": "and let's suppose Antarctica suddenly gets blown up by a nuclear war oh no it's 72% off line what am I gonna do so in a traditional bft context right you just don't finalize anything you lose any any notion of finality at all but here if I as a as a validator privately but know that Antarctica has been blown up and 72% of the proofs take validators are gone then I can use that information and incorporate it into the fortress or into the finality gadget so I can say oh I am going to reduce my threshold from two thirds to two thirds times twenty eight percent and then to me privately the chain is going to keep giving me guarantees of safety that are conditional on my own information you could potentially do even more advanced things so you could potentially have an opinion that some particular subset of alligators is more trustworthy in some ",
        "31": "way and so you you could just privately assign them more weight so there's this extremely large space of things that individual coins you can just personally do and the responsibility of figuring out what level of safety guarantees they have is sort of offloaded to the edges now there are simple heuristics you can use like for example the 1/2 plus p2 round click Oracle but if you have more information than you can use it so he gets another benefit is that you get some amount of safety with just any amount online over 1/2 right so if only half of Antarctica gets blown up and 36% of validators go offline in Casper FFG and in tender mint and in these BFC algorithms then basically they're either gonna halter they're going to stop finalizing and you lose any guarantee in Casper CBC you you ",
        "32": "still get potentially depending on what your Oracle's are somewhere between eight to sixteen percent as safety margins so you still get kind of as much safety margin as you can get under more extreme circumstances in other benefit is that this can be kind of bolted onto existing chain based proof of stake block chains very cleanly right so even just existing chains if you know teak NXT peercoin like any of any of these if they are willing to switch to LMZ ghost then the N which actually is not that much of a switch because under quote normal conditions like LM D goes tends to just agree with whatever the forest rule before was saying then they could just have nodes privately run the safety Oracle was give this extra safety guarantee add these some sloshing conditions and you have a Casper CPC system it's extremely flexible there's lots of different ways to kind of implements it and bolt it on top of ",
        "33": "other things so why upgrade to CBC why isn't Casper FFG good enough right so honest we FFG is kind of ugly and the reason why is because it kind of goes together the FFG fork choice rule which basically says highest justified epoch wins with some underlying fortress rule so in the original Casper FFG paper it would be gluing together the FFG fork choice with proof-of-work C in the current Imperium Pikachu and implementation its gluing together FFG fork choice with LMZ ghost and this has prior this kind of ugliness has practical consequences there's kind of bounce back and forth attacks between two chains and there are ways to fix these attacks but they're definitely less clean than what you've just seen from a CBC Casper right so that's about one of the kind of bigger reasons just increasing kind of complim conceptual simple ",
        "34": "and just decreasing the conceptual attack surface of the design but also all of these other benefits of ICBC that I talked about so why is the one if you REM 2.0 phase zero not using CBC right why is it still using FFG so one of the big reasons why we were not able to do this earlier was some efficiency so basically the problem with implementing CBC is that you have these two conditions that you need to enforce one of them is this validity condition I mentioned which is for a block to be valid under the Casper CBC rules the block needs to be the building on top of what the chain itself kind of thinks is the head of the chain in some sense right so the block itself needs to contain evidence that shows that the person creating the block has the right ",
        "35": "to build that particular block at that time on top of that particular parent and this requires so basically executing the LMZ ghost fortress rule in protocol as part of the validity condition of a block so in English this basically says in order for me to be event for a block B to be valid it must be the case that given all the evidence included in B or its predecessors B as the correct block to build slashing condition this is the only CBC Sailaja condition we're down to one if a validator creates two messages m1 and m2 we are m1 does not reference them to an m2 does not reference m1 then the validators deposit can be slashed so every message that you make has to reference the previous message that you made and it had the chain that you're building on has to like basically include a superset of all of that information so everything you say must reference the previous thing you said ",
        "36": "forming a single personal kind of message chain so the implication is that if you make a block that took into account some piece of evidence all future blocks must also take into account that evidence the problem is of course we have a beacon chain we have a huge number of validators that are making messages very quickly we make this practical now we make this practical in a Casper FFG context with kind of separating out blocks and anta stations so we have one block every six seconds potentially thousands of at the stations every six seconds and we use this cryptographic technique called BLS aggregation that allows us to combine together a huge amount of signatures into one signature that in terms of data complexity requires one bit per participant and in terms of one computational complexity requires one EC ad which has maybe one or a few microseconds to a per validator the problem is that BSL BLS aggregation ",
        "37": "requires many validators to be signing the same message but CBC Casper kind of goes against that right so CB TC Casper says every validators message must reference that validators previous message and so it sounds like we're requiring validators to be each signing different things which sounds like it might make be a less aggravation efficiently impossible so here is a solution every block maintains a Merkel tree of the valid of every validators most recent message and so the way that you make sure that each validator builds on their previous message is that each validator builds on a Merkel root which is the same Merkel root for every one containing everyone's previous messages so a slashing condition basically says if a validator signs a message of salaat C of a block or their most recent message was at some salat a before C and they signed another message it's a lot B that's between a and C then they get penalized so a problem is so it ",
        "38": "challenges right computing LM z goes to takes a huge amount of time there's this other version of ghost called bitwise LMG ghost where instead of going block by block you kind of make it even more finer grains and you go bit by bit in the ham in block hashes and it turns out that this has a relatively efficient time algorithm where you can store information inside of the chain about whether or not like basically the chain is valid so for every validator you can basically there's this this algorithm that you can use and you you can basically get verification of LMZ ghost in something close to kind of K times law some logarithm logarithmic time if there are K validators that are being changed for SWAT so this is actually not much more expensive than what's happening already where you have that many changes already happening to your Merkel tree so these ",
        "39": "things are definitely possible now what does this mean right so first of all casper cbc it can just be bolted on to existing systems including any form of proof of stake but also this means that casper CBC has this path toward kind of potential faster deployment in etherium anyway that actually changes surprisingly little of how aetherium itself kind of fundamentally works today right so cbc is definitely not something that's happening in phase zero phase one but it's also there's no need for cbc Casper to be a kind of aetherium 3.0 territory it's something that we can start getting the benefits of more quickly thank you [Applause] cool thanks V so next up we have someone who over the past year has been super ",
        "40": "critical and super awesome in pushing the eath to space forward so let's give a warm welcome to Danny Ryan hello I'm Danny Ryan I work with the etherium foundation I spend almost all of my time on aetherium 2.0 research specifications some client development client coordination I run some calls I try to do everything to help just move this thing forward today I want to talk to you about the state of aetherium 2.0 we have a number of awesome talks coming up in the next few days with some researchers at metallic started off and clients and so I want to just ground you and what the hell has been going on a little bit about the architecture and what we expect moving forward so the ",
        "41": "past 12 months quick recap so 12 months ago this project didn't exist we had two major scaling and protocol upgrade efforts happening in parallel was the hybrid Casper proof estate group of work in bedded into a contract and sharding via the sharding manager contract in which these tire chains are kind of like bolted on to the existing system managed in a contract both these were system level contracts meaning that they're recognized by the protocol as containing information relevant to the consensus and both them were happening really in the most the complexity was managed inside of these contracts in IBM so there were a number of a lot of progress was made and eip was written on the casper work and a couple of clients were developing that and there were some prototypes going on with the sharding SMC but we were running to some issues ",
        "42": "primarily shoes by trying to bolt this onto existing system and trying to do things doing complex things inside of the UVM a lot of the issues were around trying to process a high number of signatures in the EPM which was limiting the amount of balladeers the crew participate in these games and ultimately limiting the decentralization and scalability of the systems so we deprecated both these work streams and unified them Kasper Hart sharding deprecated them into a new much more radical design that's been deemed 2.0 is the ultimate vision of serenity this radical new design has allowed us to make more bold decisions we architect from the ground up and give us really a truly decentralized scalable block chain that we've all been looking for so mid 2018 lots of research lots of design a lot of the components the ",
        "43": "research were there but it was really this effort of kind of figuring out how everything was gonna fit together into a more concrete spec that people could begin working on a lot of work in research we had a couple of Python proof of concepts and this like hacking D proto spec if you were in a client implementer at the time it was kind of a nightmare to follow the changes there as we got towards DEFCON we moved everything into a specs repo where we had changelogs client implementing teams really were ramping up at the time beginning to participate a lot more in the conversation and build out some proto clients and we have these 200 calls we had a workshop this is a picture of workshop in Prague it was incredibly productive getting us all in the same room and super cold we're all wearing like jackets in this warehouse is a good time prototypes beginning to happen and the bones at this point I'd say of the spec we're in place but we were still in for some major major ",
        "44": "revisions over the coming months come January 31st we had an internal deadline of having a feature complete spec release this is the first release the January pre-release of the phase zero spec and I'll go into a little bit what these these phases are and this for me and for the clients was really the the mark of okay we were out of research we were into a full-blown engineering effort and really although there was a number a ton of client work going on before then most of the incredible progress has really happened since this this time point so through today there are a number of congratulations there are a number of tests nuts single client tests nuts out and a number that are in the works it's been super exciting for me personally just to be working on this project for so long and just finally see and feel something tangible super pumped ",
        "45": "this is just some dump of a I think maybe it's Nimbus actually I'm not sure of a testament the phase one specs are happening rapidly in parallel major simplifications have been happening on that front and as I said the initial client test notes and we just had another workshop a few days ago super productive getting everyone in the same room okay so I want to take a break from timeline and things and talk a little bit about the client architecture I know especially when I talk to community members they're very they're like okay I understand proof of work I understand the single-chain paradigm but I really you know I was the guy that could explain everything to everyone but but I'm really struggling to understand the complexity of this this new design and so yes when you add when you take one chain and you make a thousand of them there's more complexity and when you move from proof of work to group of stake there's also much more complexity ",
        "46": "because proof of work although it has its problems a lot of the complexities handled extra protocol and so we're bringing that complexity in with mistakes so there it is it is a little bit more complicated but I believe we can get an intuition for it and so as I said there's a lot of talks about this so hopefully I can get you grounded in it so we have the proof of work chain we have this is the chain we know and love stable chugging along has the community has a lot of economic activity this in the new system as primarily serves as the economic seeds for a theorem 2.0 initially just handling validator deposits and so moving beyond the proof of work chain we have the beacon chain the beacon chain is the core system level chain for the entirety of this new system it's where the validators live it's where finality occurs so proof of stake finality where we come to consensus on which what is the head of the chain ",
        "47": "there's an R and G in there so random number generation via the participation and commitments of different validators that's where the the validator deposits live so the rewards the penalties slashing all of that occurs their rewards being if you do your job well you get rewarded if you do your job poorly you get penalized there's also a group component to that so if we all do our jobs well together we all make a little bit more money whereas if we're all doing a crappy job we might lose more money and this notion of this thing we call the shuffling validators have a number of duties at any given time and the combination of the current validator set with the output of the RNG dictates what validators need to be doing so this beacon chain is the system level chain it's no user activity what happens here it's just this the core consensus of the system it also serves as the the backbone or maybe the spine of the chain where the shard chains are connected to ",
        "48": "shard chains are much more what we think of as a block chain at least from the user level perspective the shard chains each of these you can think of as like simply similar to the etherium 100 chain where it has accounts and transactions and user level activity these shard chains are I think I have a local these shard chains are connected back to the beacon chain via we'll call these things cross links and so one of the main jobs is a validator is you're assigned to a shard chain where your sink you sink to the shard chain attest to the availability of the data and make you signed a commitment attempting to make a cross link back in and so we take these recent references of shard chains and put them into the beacon chain now this reference does a number of things one the the primary thing it does is that it routes the shard chain back into ",
        "49": "the beacon chain and by doing this to find the head of a shard chain I find the head of the beacon chain I find the cross link the latest cross link for the associated shard I go to that block I'm now at the shard chain and I find the head from there and so it unifies the consensus of the shard chain back into the the beacon chain the other thing the other primary thing that these cross-links do is they serve for the basis of asynchronous communication between shards so if I have a user level transaction right here on shard one that wants to communicate to shard 1024 a cross link is made and then a cross link at once the cross link is made a receipt that was emitted over here because the char chain over here knows about the route of the beacon chain a claim can be made about this cross about this receipt the receipt is consumed and in acts essentially a transaction on the other so these cross-links really unify and bring the shard chains back into the ",
        "50": "beacon chain and so right so we have beacon chain we have shard chains and so as a validator what I do I have two main roles one is to create these cross-links and the other is to serve a little bit serve a little bit on a longer-term role on the shard chain so maybe every week or two I get rotated from one charge or another building the shard and creating the user level block chains beyond that we have state executions so we have the data here and then we have a layer one interpretation of that data a VM almost certainly was just some stuff to work through there okay so general architecture is generally what's going on the people are gonna be going to a lot more detail on how that manifests in clients and some of the research around the different components throughout the next few days outside the core architecture ",
        "51": "we have Co zation algorithms tree hashing which is kind of a fun little way to take any object in hashed into a Merkle tree to make proofs about them BLS 1231 and BLS aggregation networking and discovery lib p2p which is a modular p2p networking stack merkel proof formats like current protocols test formats there's all sorts of stuff that sits right outside of this core architecture that really kind of puts the clients really together cool phases development so the development of e2o is divided and broadly into three phases right now and this is to help manage the complexity phase zero is the beacon chain it's the core system mobile chain simply just a validator based proof of stake chain this is what has primeira Leon on with respect to client development so far and we do expect to launch simply this core chain without ",
        "52": "any usual activity this is for a conservative approach although it would be main net with real ethan reopen it's real rewards and penalties it does give us some allows us to coordinate in the case of some instabilities or any issues that arise without having to deal with user level activity phase one brings in the shard chains but just shard chains as data so just coming to consensus on this data being able to build these chains link them back into the beacon chain it does not have user level activity still it doesn't have a VM and enshrined VM transactions accounts the things that you're used to and this is again to grow from the center out adding complexity this is largely a networking problem and something that I expect to launch not too far after this phase zero ",
        "53": "based is to add a enshrined layer one interpretation of the data so adding accounts contracts the VM transactions data executions and the crush card transactions crushed our communications this I say the layer one interpretation because although both prior to phase one we can still use this data layer for interesting things anytime we come to consensus on data anyone layer two can choose to interpret that data however they want it's lots of cool scaling solutions you can do just with the data so those are the primary phases of this this core architecture there's some stuff we've been talking about that we want to do in parallel this is not 1x these are potential things that we want to do to improve one oh utilizing - Oh while - oh is being developed one thing ",
        "54": "that we wanted to do is utilize the 200 beacon chain to finalize the 100 chain this is very similar to the Casper the hybrid proof-of-work mistake we're using a contract to finalize the proof work chain we can use the beacon chain in the same way seems like a lot of the community members are pretty into this idea of leveraging the security of the per fortune with validators and potentially reducing rewards after that we can expose the - Oh state routes so the beacon chain state route you can make proofs via the peeking chain state route about the data and state of any of the shards so by exposing the - Oh state route in one oh we can then leverage the shard chain data layer for tons of interesting scaleable things like the one thing that's really been talked about is using it as a date availability layer for Zeke a rollup phase and these these are not canonical letters I just ",
        "55": "throw them out there but you know a little bit in flux but they see with the at least through phase zero and one there's no fungibility between these two systems between the the ether on these two systems and it's definitely something that the community is interested in so as we move along this if we are if we have finalized and we are exposing the state routes there is some there are some options on doing fungibility so if the community is really interested in it and it's technically feasible I might go down that path and then phase D is really just what do we do with the 100 chain there's a number of options one might be to roll and I won't go into detail right now too much time but roll the 100 chain into two Oh probably be a stateless contract or keeping it living as this external appendage of the system that highly uses the security of the toooo chain and just kind of lives in perpetuity there's a few there's some ",
        "56": "trade-offs here some different technical challenges and really it boils down to what the community's really wants to see progress is being made on all of these a lot of this depends on light clients - Oh light clients but again progress is made highly inflexible the path room here much work many hands that's not a minus sign that's a tilde somebody pointed out looks like a - there's approximately a hundred people working on this which is wild there are ten clients roughly of various commitment size stability etc I wasn't going to name them all but I think I'm gonna try an animal okay Trinity Nimbus lodestar lighthouse prism youth parody Geth has some stuff working oh man harmony somebody help me I'm ",
        "57": "sorry Artemis yes and then there's an executable high spec and an executable go stick so there's a lot of people touching this thing and beyond that they have we have all certainly we have blue p2p the protocol labs guys are working we have white box working on this Pegasus has a research team outside of the EF like there are I'm sorry I probably I should know named anybody because I'm not going to name everybody but there's tons of people working on this it's super exciting okay what to expect those emojis are broken sorry about that q2 we're gonna freeze the phase zero spec clients are gonna be stabilizing and optimizing we're gonna see more single client and multi client probably short-lived test nets and continue this phase zero spected element upcoming client challenges in this quarter and ",
        "58": "the next really clients are the unsung heroes of this they're taking these like crazy ideas and making them a reality and to take that and like make it a production client and like a production piece of software is like a huge undertaking so they're gonna be passing Consensus Estimate optimizations up to four million validators efficient aggregation strategies both in terms of computation and bandwidth they're gonna be seeking stable networks they're gonna be doing state sync strategies working on validator ui/ux security reviews visualization tools the list goes on there's a lot of work to do they're doing the hard work if you please thank your client developers and honestly fund your client developers q3 audits and formal verification cross client CI and fuzzing as we also increase just the tests vectors would be doing some network stress tests start to see long-standing multi-client test nets and the phase 1 and phase 2 spec development ",
        "59": "somewhere I'm not gonna claim when we're gonna freeze phase 1 spec development but it's gonna be getting pretty stable and at that point might have a couple of clients do some prototyping q4 towards phase zero lunch somewhere definitely around this time we will be coming up with a launch plan and enacting it there's some unknowns between here and there and instability of networks but we're gonna be getting close to this point phase 1 I expect to be stable and in development and phase 2 the spec will be maturing TLDR it's happening it's super exciting if you want to get involved talk to me talk to the client there's a ton to do and thank you thanks Danny I feel much more knowledgeable ",
        "60": "about the wonderful crazy world of E - so next up we have yet another researcher it's Nerium foundation rock star who's going to be telling us about a life in the day a baby can change validate validator so let's give a round of applause for Xiao Wei Wang so today we really have to quantify talks talk about the eastern 2.0 architecture and row map and my talk is about other and shall one final foundation research thanks painful the introduction and my talk is about the life of an interim potential validators so it's about the life cycle and what's the responsibility of the retention validator ",
        "61": "so um a quick Rick we are erasing this diagram before and my talk will focus on the peak and trim parts so the peak engine is a whole new trend here it has to be joining from the east one trim contract and it has its own Genesis from in the pretension and that's Rome app I will focus on the beacon trim phase zero and see that what we already have and how we will do so first crashing is that why would people wants to join the pension and become a validator the ",
        "62": "answer is simple because there is an incentive of the rerolls for the validators get so the reward includes that so first it's similar to East one - to be evaluators you can include as much as many transaction as you can so the there are two currently you have two types of the coal that operations can make the can get the free fall and why is the transfer and another one is we call add up attestations so attestation is you can see that us just votes you can vote for which you quit block you think last of valid block and in this authorizations it includes the ",
        "63": "aggregated signatures so but for the individual very data they have to assign for some specific attestations so once you actually snipe for that message you have to be responsible of that if you if you sign something that incorrect you will be punished later okay so that's the decisions so the validators they can get rewards from include as many a high station as they can the second rewards from the for the processors is that they can become the whistleblower sock whistle oh is that so as I said that sound very deters some dishonest validators blame my sentencing invalid so if you are honest very latest you can to report to the other ",
        "64": "evaluators that hey nice guy he did something very bad and please let's stash it so once this stashing message beep brooke has into the network and then it can't be included into the block then the money if either from the ash the data will goes to the whistleblower so Matson mechanism to prevent that people to do to the passing and to incentive that people to to to be justice and to make those Chen safe and the second kind of reward is the FFG world so it's about that you have you are really both photo correct in well correct value chain of the potential and ",
        "65": "there are certain of the rule is we call that cross link rewards Crossing is the link between the Shan Shan and potential so when the attestation the violators name makes attestation at the same time they also makes cross link so let's cross single link to the churches and to to attach that with Shao Qiang you are validated and is that which the plaque of the shop block is valid in your view so those are the three types of rewards in our current spec that can make a redditor to get reward and you know now you know that why we want to become a very Gator and second question here is that how to become validator so how to join the ",
        "66": "staking so Larry will be a spatial contract on the east one chain we call that the deposit contract so every user they can make a deposit of 30 right now it's the story through either to this special contract and after they make this passage the Predators can watch the logs the event logs from the east one deposit contract and once the deposit already be met into the contract it will emit special locks so once the data that can be a like kind of the East one chain and then listen and watch less logs to see if my deposit has already been except in the is one chain and also if ",
        "67": "other deposit other editors are joining the nutrient okay so um if there's a lifecycle GraphLab we assume that the violator already be a deposit from Indy is one chain and also we have to make sure that it did deposit enough balance and it's ready to be active well note that not the visitor doesn't be active immediately once they a deposit they need to wait for like from right now it's the 25 minutes lease for to be active fit then the Serpa is that once a Revelator's activity leprechaun we call a layered active validators and ",
        "68": "active validators will be adding to a long very data registry list we mean please know that it has to be one single huge list because we aren't going to make the shooter assembling from the one single of idea to poor that's from and and as we talked about previously this very importance from in the potential Carmona is the randomness Sarandon is provide to have the ability to make pseudo-random e simple of the fighters travelling function and the surprise is that why should the validator do in this context so um right now and only talk ",
        "69": "about is the responsibilities in phase zero they have two men jobs to do the remember one is they have to propose the very valid beacon block and second since that has to create attestations so um this is the current architecture so a current data structure of the beacon bug we have now two notable things in the black body why is the render review in issue we now use the render to provide the randomness in the season and and every proposer in when they proposed a block they have to provide their own render review and that will be the part of the entropy to create the ",
        "70": "randomness seed that's how the randomness goes and the second part is that they have to choose the best world of the east one chain reference let's the link between the is to chain into the is one chain here so the weighted as I say that the visitors of the pecan shrimp radiators they have to be kind of the like hi after is one chain that's the link between them and also they have to increase the return operations in the block in the pecan black body is here now we have different types of the operations that are the list is like proposer stashes the stash messages and attestations also the deposit message has to be included here and then we have the how to ability exceed it will be ",
        "71": "message into here okay and the second important thing is to create a test ations the test a shoes message has three parts things folder refactoring by danny is much more better now if you are client apps and they have three parts that one is the they will have to vote for three sting why still they have to vote for the east one trim block and second one is that i have to vote folder P comeback and third one is the Shantaram block we also call that a course link so yeah so you can see that because the way that the pecan survey data they are in the core layer between the mention the is one mentioned and the xiaochun's so they have to be the bridge between them ",
        "72": "okay and the incentive is not only the rerolls there are also the stash - off to the Inti's - the dishonest redditors to improve them from being malicious there are different types of their penalties why is the referee penalties so if the very dangerously contribute to the FFG realisations penalty they will get rewards but if they didn't they will get punished then there's also the inactivity liek staches penalties that's for that if the chant doesn't being ",
        "73": "penalized normally that's because like maybe we didn't get the three so it holds like two thirds so that time we will punish the data's that they didn't delay job well surprise is the crossing penalties it's similar to the profile is focused on the xiaochun of the votes and there are also the two types of the slashings wise for the proposer and another is for the tester and also the provoke a stick and penalties so after me adjusting we'll talk about that deeper okay then let's see the verminator Chun and it's driven by the total balance here we have some ",
        "74": "calculations functions that to calculate costume ex-felons from we have right now so right now in the version zero five one zero point five one space there are two Q's in the mass spec Y is the activation waiting to another is the withdrawal q but Lera working progress PR hereby Vitalik his suggestion we can use the execute instead of the withdrawn Q here and how and then once the vanator stop being active being doing a job but what about one thing they just don't want to do it anymore they want to exceed how can they can ",
        "75": "they do that yes good news is that we have the battery exit message here so the visitor they can send the mental rear exit operation to the network and if it's ham it would be a message that be included by the black browser and hum about the withdrawn that most people care about unfortunately in fest zero we are we only can make them into withdraw Ball State because because for now there's no State marching in the beacon chain but good news is that infest true we will make them with drawable found with drawable state to withdrawn and the editor index will be recyclable here okay so let's see the tab again so now the ",
        "76": "visitors are already being activate and on there are three types of the first lab three types of the rustling my to Weiss not lame I got stashed and if the gas - they will be XA - so there you can notice leather um - stable the the violator the active to stabilize the activators we the size of the active radiators should be kind of stable you can be updates to frequently folder they work security and then so it's so that's why the radius don't jump immediately you have to wait for a while time and then the carstache and exceed state and after like 30 36 days they got with ",
        "77": "drawable state and the second type is dialect make God a church kid because they don't have instant in they don't have sufficient balance in their account that's a similar case Dahle will wait for four epochs twenty five minutes and it got stashed on stash but lakes it anyway the for a while to be inter withdraw book you and they can get with durable state the surprising stubborn hurry exceed case but I'm busted she's not allowed that the the tag one might be updates later after the walking progress PIB merged so let's hearts the current value later status would be transition and also the the on stash and ",
        "78": "exceeded state might be go up to another path because of yeah while they are waiting for being exceeded a my housing my changed between when they are waiting for okay and I want to talk about that sound computation and network requirements in our currencies that here so um hospitality already said that mentioned that the worst case inside we have the four million registers in the beacon state so um imagine land on for millions of a data in your single array and we want to make the community shopping of this four million array so ",
        "79": "it takes time and also we have to improve the beacons state cash management of course because two updates the state of the very data's will be hard then in the huge case another issues here is the networking publication so we have different types of the messages like the we can plot the attestations and other transactions so um some of this message needs to be broadcast to the whole beacon train universe but maybe some message like attestations it only needs to be applied to be broadcasted to the shot network with hope so imagine that we have like ",
        "80": "1,024 xiaochun's attractions has their vocal messages for the shop blocks and a test authorizations has to be a cricket in the shop that they work because um if it will work has lost all the message to the whole network european - no it's in network and also as we said that we want to stay both shot invaded her shoveling but - for the security the radiators has to be assigned to different shot committee after a while so but also it doesn't happen it still needs time to like wave it over later in the shower I want to switch to ",
        "81": "ChaCha but when I want to the switch between the network's I have to think the new things from the xiaochun a1 a2 chattering - so how to do the fasting immediately that's a question we are going to think about deeper and the Serpa is the bill a signature aggregations so right now we expect nirupa 128 validators in each committee but if we think about the worst case there'll be 4 millions very daters so to the basic calculations in the worst case not narrow P like 6 fat 65 thousands of the individual signatures per seconds so of course that we need to aggregate the signatures so let's another hard ",
        "82": "question in engineering that we want to solve and if you are interesting in the POS there are working progress Peck but we are going to but it's um is a perversion and we want to standardize this in the few months speaking of the stabilizing the specs if we have liras 3t from men top when kind of the specs why is v 0 specs including the correlations SSD and POS signatures there are also two other working progress things in our mind but there is the networking specs thanks found a mat from coke home Liam very helpful for us and also the editor ",
        "83": "interface spec was thinking about it so yeah but we are walking right now so um if we're interesting in contributing and happy to chat with you on the Peter and also I'm working on the charity the Python client implantation here thank you Thank You Xiao Wei that was great I think I've got to start saving up so I can have 32 weeks and become a validator so last but not least in our trifecta of Awesome aetherium 2.0 researchers I'd like to welcome to the stage to talk about the east to custody game Justin Drake okay thank you ",
        "84": "okay so I'm going to talk about phase one of a film 2.0 which is the data layer and specifically I'm going to talk about the data availability problem which comes with having lots of data and even more specifically I'm going to talk about the custody game which is one of the solutions to data availability so three parts in the talk what is the data availability problem what are some of the solutions that we intend to deploy in in phase one and in the future and then a deep dive into the custody bit game which was recently simplified okay so what is the data availability ",
        "85": "availability problem so basically you have data and you can reference that data where some sort of identifier which is six things like a 32-bit byte hash and the question is if you only given the hash and not and you don't have the data is is the data available to you can you actually go ahead and download it and one very like simple approach which is kind of the naive brute-force approach is that you just go ahead and try to download the data and then you will know whether or not you can download the data and that's perfectly fine for from 1.0 and for the beacon chain because these are you know very small chains and every valid data which assumed to be you know a laptop on the on the home connection can can download without any problems but this is unscalable this kind of naive approach especially in the context ",
        "86": "of shouting so even though one single shard has totally reasonable bandwidth you know let's say on the order of one megabyte per minute once you have a thousand shards then it becomes a lot more data on the order of one gigabyte per minute and so you know how do how do we deal with all this data and like the scalability solution of a firm 2.0 is that everyone doesn't download all the data and it's just there's just too much data for the battery days is download so what we do is that we take the shots and we kind of segment them into into a box and then for each epoch we have so-called cross-linked data which is going to be the data in the shards plus the data in the headers and this cross link data that is 16 megabits which is 2 megabytes and the reason I'm talking in bits will be will be relevant later and what we have is that we have these references from the beacon chain to to ",
        "87": "the shard data so we have these so called cross links which are the yellow stars and the hashes which are the the purple rectangles and they're pointing to data and so as a validate of the beacon chain you only see these very 16 cross links and you want to have high guarantees that the data that these cross links are pointing to are indeed available okay so let's not look you know too much as to what's in the data we can forget it's you know blocks and headers let's just assume it's a blob of data and what are the steps to kind of creating a cross link in FM 2.0 so the first step is to mark relize so we have this hash tree root so instead of taking a simple hash we we mark allies so just at that detail we have this identifier which is a data root and then the DIA test is we ",
        "88": "basically validators in a committee assigned to this very specific cross-linked data are meant to download the data and and sign it if they are able to sign it so they kind of out saying that the data is available is downloadable and then the various attestations are aggregated for across the committee using BLS aggregation and then once you reach a certain threshold which is enshrined in the protocol then you get the the yellow star and you are a cross link and these cross-links go in the beacon chain and so you know to rephrase the data availability problem how do we know that you know from the cross link you can recover the data if you're not a validator that you know was assigned to this specific cross link and the reason that you really want to know that the data is available is that if for some reason or another this data is ",
        "89": "not available then that means that you can't download it the in general can download it then you have a so-called unavailable cross link and this is like super toxic waste that will pollute the beacon chain extremely fast and so if if this happens then you know emergency mechanisms needs to come in place we need to do manual roll back all the way back to when when there wasn't this unavailable across link so this is one of the the main problems of phase one and we're going to see how how we can solve this problem so in phase one we're gonna have three solutions to to the data validity program so one is going to involve cross linking which is a process it just walks through but I will give more details as to why it works and then ",
        "90": "we're going to go through custody proofs which is like the the main content of this talk and then there's also a mechanism which is the chunk challenges and you can kind of think of them as kind of on a time axis where the custody proofs are proof that you have the data and you can only build them when you have the data so they're kind of before even committing to having the data so that kind of in the past relative to the cross link cross link is like a real-time thing you know you're committing to having seen the having seen the data in real time and then the chunk challenges is basically a thing that happens post factor which is oh you know use you committed to this data please give me this this random chunk in in of the data that happens post factum so and then in phase 3 a plus or maybe in the film 3.0 I don't really know or maybe even in phase 2 if if if we're very aggressive ",
        "91": "we have something more fancy which we call availability proofs and these use kind of fancy math and tricks like racial code and Starks and I won't go into details that's I guess for never talk okay so let's see why cross links work so the reason cross links works under a certain secure security model that we have any theorem 2.0 namely that you know an honest matter it's the assumption is because we have this this pool a very large pool let's say a million validators and we're assuming that two thirds are honest so they will follow the protocol rules which includes downloading the data so to further honest to further angels and one further demons and when you have a pretty good random number generator such as R and L then and you have a a cross link ",
        "92": "Committee which is around a thousand validator stand with extremely high probability at least one half of the validator is honest so you leave you lose a little bit of honesty going from the pool to the community but it's not too bad and then because you have this fresh hold in the cross link you can make the threshold large enough and so in this case you make it greater than 1/2 and then you have a guarantee with a very high probability that at least one if you do reach the threshold at least one of the voters will be honest and so being honest means that you've you've actually have downloaded the data it's on your computer if someone requests it's from you your your share it with them so the data is available to the whole network okay but there is one problem with cross links and it's it's due to this kind of misalignment of incentives so there's two ways to go ",
        "93": "about cross links so the the honest way is basically you have this chunk of data which is quite large and you need to go ahead and download it then only after downloading it you you sign your attestation and then because you did all this work we will reward you in the protocol and this is kind of the approach what you need to do all this work and you know you have this man that's sweating but then there's the other approach which is kind of cheating which is you have other people that are making votes these data stations and you just kind of look at what other people are doing and you kind of copy what you're doing so it's kind of a copycat strategy and seeing what other people are voting on is very cheap because these votes are very succinct and so you can just follow the crowd and still get the rewards and this is basically the light node approach which is the lazy ",
        "94": "approach and it's kind of like SPV mining in Bitcoin and it's it's kind of bad because if you have an attacker which is not necessarily that's huge but like significant they can they can vote for an unavailable cross link and then all the lazy people will pile on and then you'll reach the threshold and then you have an unavailable cross link so that's that's kind of bad so one of the key idea is that in addition to signing the data you're gonna tag on this proof of custody which is this kind of really cool construction which is just you're going to tag on just one single bit over zero or one in your attestation and that is going to prevent or disincentivize people to be lazy because if people are lazy and they don't download the data then with probability 1/2 they're going to mess up ",
        "95": "the proof of custody and so they're gonna get slashed so as a now now as a validator if you want to be lazy what are you gaining you're just gaining a little bit of bandwidth it's a very minimal gain but what do you stand to lose you stand to lose your deposit with with with high probability and what we've done basically here is that we've upgraded the security assumption of cross links which was honest majority to something closer to rational majority so there's this kind of this is the the honest people and then there's the rational or the kind of two extremes and there's there's all sorts of things that go in between honesty and rationality so one one of the things in the gap is going to be laziness and that's specifically what we're targeting I mean there's other things that can happen by collusion and bribing which we're not addressing here but laziness is is what ",
        "96": "we are addressing ok and same thing here with extremely high probability you're going to have at least one on this on this but lazy a validator and they're gonna actually do the work because of the proof of custody okay um chunk challenges these are very easy so let's assume that you've signed an attestation so you've committed to the fact that you've downloaded some data and you know the data someone might want to make a query about the data and they say just give me this chunk at random and the the response needs to be fed or on the beacon chain so everyone is a is is aware of that you can chain and the beacon chain itself is available through this brute approach of everyone that just downloading it and the reason why we have a small chunk as opposed to the ",
        "97": "whole data is because the beacon chain has very limited bandwidth and so we can only afford to put these small chunks and then this in addition to the small chunk you also have a merkel proof that the chunk has indeed match the cross link and the the index basically the location in the data and that has been that was the challenge okay so now to the the most interesting bit which is the the custody bit game so how do we basically enhance these data stations by adding one single bit in such a way that if you don't download the data you will be caught half of the time so just to reframe the problem again slightly more exactly so you have the data you have the data routes the signature both for monetization and you want to basically have a so-called custody bit which is a basically a ",
        "98": "crypto economic construction so it's not a cryptographic thing is not like you know cryptographically with very high probability that that person has downloaded the data you you basically weaken that a little bit and say that we probably see one half if they got it wrong so it will probably one half they will only get it right they only get the custody bit right we've probably see one half and they'll get it wrong with probability one half and we will know about this at some point in the future in the in the near future okay so just a quick note about why one one one single bit is kind of a very nice property to have and the reason is really related to aggregation so VLS aggregation is signature aggregation techniques technique which works extremely well when you're aggregating the same the same message so the the verification ",
        "99": "cost technically is like going to be like one pairing per message and doesn't matter how many people are signing it you still have this this basic cost but as soon as you have multiple messages being signed you need to pay these multiple pairings and when you only have two types of messages you know attestation plus the bit one attestation plus two B two then you can just aggregate them separately and your verification cost only doubled and it's it's still it's still marginal so basically in the in the optimistic case where people get the proof of the custody bit correct then there's this there's very little overhead to the beacon chain and if people are you know are doing very stupid things like not downloading the data but still participating in the other stations then they'll get caught and there will be a challenge game to basically slash them okay so what is kind of the one of the ",
        "100": "key ideas in the construction so you have the data and you want to come up with a construction which kind of proves that you and only you have you definitely have the data so one of the ideas is that you introduce the notion of a key which is unique to you as a validator so even though the data is generic to everyone you have a key that is unique to you and we're going to temporarily attach the value to this key so if you give the key to someone else to some sort of outsourcing service then that outsourcing service can can use the key and take your money so if you don't trust the outsourcing service you really don't want to give them the key your key okay so we have the data we have this private key and what we do is that we mix the data and the key really really thoroughly so that you know they kind of maximally combined and then once we have this this mix which for example could be ",
        "101": "the XOR so you have the very long piece of data and the short key you just XOR every single chunk in the data that's one way of mixing and then once you have this mix you can extract one single random bit so that's going to be using a so-called pseudo-random function the PRF and one way for example to dragged around in bit is to take the hash of the mix and take the first bit of the hash it is going to be random and the whole point of this construction is that the only way to compute this custody bit is to have both the data and the key and in particular to have the data so you have downloaded the data and just to to rephrase this no no no possibility visually you have the validator they have the key but they kind of want to be lazy they wanna outsource downloading of data to someone else so someone else has the data and unfortunately in order to compute the ",
        "102": "the custody bit you need to combine the key and the data so there's kind of two ways to combine from this from this setup way number one is to take the data and bring it to the left side well that's kind of defeats the purpose because now the valid data actually has to download the data so they've gained nothing from the outsourcing service and the other option is to send the key to the right side but that's not going to work for the validator because then the outsourcing service has the key and so they can still you can see all the money up from the validator okay so let's go through like the whole dance in the scheme in the protocol so we have three phases commit reveal and then challenge so commit you commit to having data then you're going to reveal your key and then if the custody bit is wrong you're going to be slashed so off chain in the commit ",
        "103": "phase you will generate your key which is going to be unique and deterministic you download the data you complete a bit and then you commit boom that goes on train some time later you will reveal your key and then once you've revealed the key basically your custody bit becomes publicly verifiable anyone anywhere in the world who has the data now now also has the key and so they can verify off chain you know does the does the mix as the combination of and the data matched the custody bit that was in the initial commitment in the attestation and if for some reason it does not match then then you challenge you you go and chain with the data and you say hey we can chain please you know compute this disgustedly bit it's wrong and and please slash slash the validator and in addition to this ",
        "104": "slashing condition we have another session condition which I kind of been talking about explicitly which is that if for some reason the key has been given to someone else then you want to have this the slashing condition for leaking the key early before you met before the reveal phase okay now there is one problem with this the scheme here and is that in the worst case if validators have a bad custody bit then the in the challenge the the data is going to be is going to be like the whole cross link data it's going to be all 16 megabits so that's too much data for the beacon chain the beacon chain can only handle things like on the order of so-called chunks which you know are about 5 4 4 4 kilobits 4000 bits so how ",
        "105": "do we go from this very large piece of data 16 megabits to for just four kilobits and the trick is to basically take the data and cut it into four thousand chunks each chunk of size four thousand bits and then for each chunk you compute a separate custody bit and then you kind of combine these bits the so-called chunk bits you have 4,000 of those into a final custody bit so if you just ignore the middle bit you still have a custodial PRF so you help still have a scheme which takes the data and produces one bit and guarantees that you have the day but there's this now there's this kind of intermediate thing in the middle and this intermediate thing allows you to challenge very efficiently so if for ",
        "106": "some reason you disagree on the custody bit then you will publish unchain your version as a challenger of the of the of the intermediate bits and then the original validator that it's dead duty to either well to point out where the Challenger is wrong so that they can come to agreement as to who is wrong and if and if for some reason the Challenger is is not wrong then it will be impossible for the for the original validator to point out an error in the Challenger and one kind of important detail here is basically the the numbers that we've chosen so notice that four thousand is the square root of 16 million and so it turns out that taking the square root is most efficient because the yeah you're gonna have the number of chunks times the size of the chunk is going to be the size of the ",
        "107": "original file and the challenge is going to be the sum of these two things and so taking the square root is optimal anyway it's in detail but yeah just to rephrase visually the how the challenge game works the only difference is now in the in the final step in the challenge you provide these intermediates chunk bits and then the risk the validator the original validator has to point out where the the Challenger is wrong if it is wrong and the way they do that is that they provide the data to the beacon chain with the data and the key the beacon chain can compute this intermediate bit and of course the the data is fanta cated against the cross link okay so this is one of my last slides so this whole this whole scheme is I pretty pretty amazing it's very cool that it exists but one of the one ",
        "108": "of the design considerations for a firm 2.0 is to be so of MPC friendly or friendly just to sticking pools and things like that so we we want to be able to take the notion of validator which is kind of a logically a single entity and behind-the-scene kind of off of chain we want to allow users to take this validator and split it up into shares and have you know mmm event logic so there's two use cases for that one is taking pools so if you if you don't have the minimum amount of Aoife required for example 32e for 64 if for sure you can pull your funds with other people and you each have kind of voting power and proportion to your stake and then you can set a reasonable threshold within your second pool and and basically have one single validator with multiple users ",
        "109": "behind it the other use case for being embassy friendly is the idea of distributed staking so you you are a single validator you have the 30 to Eve but you don't want to put all your eggs in a single machine so what if this machine gets hacked or what if this machine goes offline so what you can do is you can for example have three different machines and you can set up two out of three validation so that if you if you have two of the machines that are online then the system keeps on running and if you have only one at most one machine that gets hacked then your your-your-your hot key which is your hot validation key contact okay and in in order to kind of be NPC friendly we need every single part of all the way from 2.0 to be NPC friendly and this is going ",
        "110": "to be three different things that are specific to the to the custody bid scheme which makes it tricky but turns out these really nice constructions that that involve on the one hand TLS signatures so it's mm you know BLS is very good for these threshold signatures and you can use fresh hold signatures as your your key scheme your secret and we do the same thing for for R and L so you know Rando is a commit reveal scheme where you could you commit to something and then you reveal your entropy one way to phrase this one once you implement it is you commit with your Els pub key and then the way you reveal is by signing a message and because BLS signatures are all unique and deterministic you have a well-defined reveal anyway and the other cool construction is for the PRF so it turns out that this is wonderful like ",
        "111": "mathematical construction called the lujon symbol and it seems to fit you know exactly our needs so it can take a chunk of bits and it will generate a unique random bit out of it and it's the whole thing is like very MPC friendly I think it's a maybe two or three rounds of communication which is great anyway thank you this was the whole scheme if you want to read the specs I encourage you there's a there's a draft right now but it's it's pretty good and yeah also want to thank people who've been working on this back and that's it thank you [Applause] awesome Justin thanks so much all right guys so we're almost off to lunch before that I have three quick announcements ",
        "112": "that we're going to make so one of them is that this is very cool I've never been to a conference that does this before but if you guys run into awesome people at this conference or you are very excited because you're you know get to share a room with people you don't always do that with and you're hoping to have a place to hang out it turns out that after the conference sort of official event ends the rooms are still available free to use so if you go to the registration desk and talk to them about that you can I think sort of like reserve rooms to hang out after that's pretty cool second thing speaking of like workshops at 1:30 p.m. today Khyber and Sealand are doing a workshop in the sub conference room that may be two workshops actually I'm not positive so feel free to go check that out those will be super exciting and lastly a very small thing the organizers have noticed a lot of luggage floating through another cool thing of this venue it turns out to offer is like it's checking ",
        "113": "so if you go downstairs there is a complimentary bag checking so you can have your bags in a safe place and don't have to clutter the halls or worry about them getting stolen and with that let's eat some food so baked for lunch you "
    }
}