{
    "metadata": {
        "video_id": "EFfqCpHmjiM",
        "title": "Closing Keynote: Vitalik Buterin - Data Availability Proofs",
        "link": "https://www.youtube.com/watch?v=EFfqCpHmjiM",
        "duration": "41:34",
        "channel_name": "StarkWare"
    },
    "transcript": {
        "0": "oh no I'm not sure if I kind of closing remarks as the best title for this presentation it kind of feels a bit grand and really all I wanted to do is just like talk about fraud proofs indeed availability proofs but here you go fraud percent data availability proofs so I guess what I kind of said set the stage for this technique the idea here is that if you look at blockchain protocols like Bitcoin etherium and specifically full notes in those protocols as they exist today there is some protocol guarantees that get broken if say more than fifty percent or thirty three percent or twenty three point to one percent or whatever number of a nodes stop following the protocol so you might have 51 percent attacks you might have finality failures and so forth but there are a lot of property is that you ",
        "1": "still keep even if arbitrary 51 percent attacks or like arbitrary dishonesty of nodes is possible right one example of this property is the property that all the blocks in whatever chain you're gonna accept as the main chain are going to be valid right and what this implies is that also people can steal your money people can't give themselves money the the coin supply can't get inflated like applications can't get randomly screwed around north and a whole bunch of other things right another guarantee you have is that the data in the chain that you accept is going to be as available so this basically means that whatever chain you accept as deleting chain well if you wants to download any particular piece of it you will be able to and this seems trivial in kind of non-scalable blockchain land but it's actually very non-trivial for some reasons that I'll go into and what this guarantee ",
        "2": "basically gives you is it gives you the ability to I mean first of all be able to ensure that the block things are correct and second to be able to kind of do things like generating new blocks like reading new transactions and it's just in general like figuring out what the hell is going on on the chain so they're gonna get directly into things so let's start talking about fraud proofs I mean so the setting here is basically suppose a malicious minor malicious proof of stake validate or whatever publishes a block and we're gonna assume all the block DNA is present so there's some magic Oracle that verifies that all of the data in the block the full body of the block has been published through the open Internet and at least one honest node has it but some of the block data may be invalid and so what we're gonna do here is we're going to design the state-transition function so the ",
        "3": "function that basically takes an existing state aplomb processes all the transactions that puts a new state in such a way that any specific invalidity any specific error in the block can be proven to be an error to a light client by basically just providing a small subset of the Merkel tree right so I mean who here is familiar with how Merkel branches work yay so okay you know homework will branches work and so basically like if you have a transaction the transactions processed incorrectly then all you need to do is just like provide a few leaves that touch the transaction let's hash the accounts affected by the transaction and you can prove that something invalid happened once someone's made a fraud proof they can broadcast it and any clients upon receiving a fraud proof can immediately verify that there's something wrong with the block so with this technique what you can do is you can basically create a system where unique clients can download all of the ",
        "4": "data so if you're a five availability but after they've downloaded a fail on all of the data they don't need to process and verify all of the data themselves instead every client might just randomly choose to process and verify some random 0.1% of the data and if any client discovers that something somewhere is wrong they can broadcast a fraud proof and once you broadcasted a fraud proof that fraud proof can get a traverse through the network other clients can verify the fraud proof and they can if I something is wrong and if you've downloaded the data and no one has sent you a fraud proof for a while then this is evidence that the the chain is correct now under what assumptions does this break number one it breaks if the number of honest nodes in the network is just extremely low so in this case it's like under under a thousand and so there's just data that no one ends up checking over and the second condition under which it breaks is if network latency is extremely high ",
        "5": "but notice that this does not break even if 90% of nodes on the network are malicious right so we've really kind of cut security assumptions for verifying validity of data down so here what we've done is we've basically reduced the amount of computation you need to do to verify that some data is valid but what we haven't done is reduced the amount of data that you need to download so data unavailability right so this is enough the big problem with probably the hardest problem in blockchain scaling and the problem basically is suppose that a malicious minor validator publishes a block and instead of the publishing the entire block with some mistakes in it what the attacker is gonna do is the attacker will publish a block where parts of the block are just not published so the attacker will publish most of the Merkle tree but not all of the Merkle tree and the parts ",
        "6": "that aren't published well they could be valid they could be invalid but both cases are attacks so you if we're gonna think about kind of proving data availability we it also kind of mentally helps to separate out the two cases so we can think about a model where we don't really care about any notion of validity and if we do care about validity well we can kind of separate we have a fraud proof mechanism for it let's just talk about a chain where all the chain does is it holds a bunch of data and a valid block is a block that has data the data could be anything but the data has to have been published it has to have been available to the network and be available and the kind of more slightly more formal way to think about this is that for every chunk of data at least one honest node has has that chunk and so if you wants to download any particular chunk then you can't ask the network for it and get it if you want to so basically the problem is how does a ",
        "7": "client to verify availability without downloading all of the data themselves now you cannot use fraud proofs for this right you can't just publish a message that says hey guys I think chunk zero is and is unavailable because assume and as soon as you publish that claim well what a clever attacker can do is they can just not publish that chunk until they see your claim and then they just immediately publish the chunk and they're like hey this guy's an idiot like why is he claiming this data is unavailable here you go and then the attacker could just keep doing this again and again and basically eventually either people stopped trusting these claims and the scheme breaks or the participants get dust and they have to download the entire data so you can't solve the availability the same way that you can so that you can detect faults in data where all of the data is available so easy solution one check everything for OFC data this ",
        "8": "obvious we take so see bandwidth it's okay for existing block chains but there is plenty of contacts in which it's not okay context one higher-capacity chains so that's a theory em today a theory em after the Istanbul heart fork that all reduced a call data gas cost from 68 to 16 the correct right chain and um sharded aetherium also with with its thousand and twenty four chains also plans to be able to it points very high right now in the grand scheme of things a few megabytes a second isn't very high like it's still within bounds for at least some home users and at least some context to be able to download everything sometimes but it's definitely too high for people for most people to reliably download so that's case one keys to and this is a case which is relevant to existing non-scalable block chains including like pretty much everything ",
        "9": "that exists now quick verification of history right so suppose that you're sinking a chain for the first time and or you're you've been offline for a long time and you wants to just get caught up with the history and you want to verify all of this stuff so you want to establish this guarantee that the data is available and let the and that the data is correct in sub linear time so in some linear computation complexity and in sub linear data complexity now right now what you have is you have like clients and the white clients can find the correct head in the very sub linear time but only if you accept its trust assumption that says that the majority of the of the miners or the majority of the validators are good let us try to remove the assumption that the majority is good can we still keep the guarantee and it turns out that yeah if you're willing to accept these assumptions that about ",
        "10": "either like Network latency and at least some minimal number of honest nodes existing yes you can so light claim protocol for verifying data availability so suppose you have a block and we're gonna just take that block in and we're gonna morkul hash it and think of the there being data at the bottom could be a few megabytes could be higher and it just gets merkel hash then you have a more covert so one simple protocol for probabilistically checking availability is is a white clients I'm going to privately select 80 positions and I'm just gonna ask the network for each one and I'm only going to accept the block as available when all 80 of my queries get answered by someone if only 70 percent of the queries get answered then I'm just gonna sit there pretend the blocks not there and if an hour later there were the last 10 queries get answered then I'm gonna accept the block an hour later right so ",
        "11": "this is kind of similar to what happens already where you only accept a block after you've downloaded the whole thing but here instead of trying to download the whole thing we're just gonna focus on a particular 80 positions so the idea here is that this is not a good way to verify that the entire data is there because there could be just one chunk that's missing and if there's only one chuck that's missing there's no way any of these clients are going to catch it but what it does prove is it proves with a very high probability that more than 50% of the data is available right so now notice that an attacker can trick specific clients so when an attacker could do was when an attacker could by default publish nothing and then just wait for these queries to come in and only publish specific chunks of data responding to the queries and then stop at 49% but basically if the attacker tries to do this first of all there is a ",
        "12": "balance on the number of clients the attacker can corrupt or a trick because eventually 80 chunks multiplied by a bunch of clients is just gonna add up to half the data and also you can make it even harder to trick specific clients basically by having all the clients make their queries with Onion Routing and so you don't know which queries to respond to in order to and if trick one specific client so this is the like client protocol right basically just like check et random positions and your probabilistic wave you are applying that at least half the data is there problem of course what if the one position that the attacker has not published is an invalid transaction that gives themselves on 91 bajillion coins so the ratio coding what we're gonna do is we're gonna load agree extend D and we're gonna load agree extend it to a larger team so now any 50% of D suffices ",
        "13": "to or any 50% of D prime suffice us to reconstruct all of D prime and so verifying 50% availability of D prime suffice us to verify 100% availability of T prime do people get this so this is this is a erase your coding right now there's one problem here which is what if the original D prime is constructed incorrectly so what if it's not a low degree extension what if the data in some places is just totally random what if at position 5 the Merkel treat doesn't even go down that many levels what if one of the chunks has 34 bytes instead of 32 then what if there's crazy stuff happening in a few and our prime actually isn't a kind of faithful low degree extension or D prime actually isn't a faithful low degree extension of D so by the way ours demarco root of D ",
        "14": "and our prime is demarco root of D Prime so you might think well right no problem if our prime is constructed correctly but if it's constructed incorrectly what do we do so one approach is to just bring back fraud proofs and we'll make an ie fraud proof and then a leaf fraud proof is pretty simple basically if you as a full node that's or at least as a node that is full with respect to that particular block so you actually may it try to download more than 50% or all of D if you've only managed to download somewhere over 50% of it but not the whole thing then you can use that 50% to reconstruct all the data and after you've reconstructed all the data you can basically just make a big huge fraud proof that just shows hey if you take this data and reconstruct it and make a merkel word of the reconstructed version the merkel root is not going to be the same as the morkul root of the original data right so basically we're just ",
        "15": "making a big huge fraud proof containing all the data you have and verifying it just requires running this entire computation extending the data and proving that it's inconsistent so this works but the problem is this makes a big frog proof so in the case of aetherium 2.0 cross-links for example in the worst case when i'm d prime can have a size of a gigabyte and so you really don't want gigabyte along fraud proofs so what do we do so the solution in this paper from last year by myself and whisset Mustafa some basically said we're gonna have to dimensional array sure codes right so what we're gonna do is instead of just extending D to D Prime along one axis we're gonna make D be this kind of little little square on the left and then we're gonna interpret D as being at a bivariate polynomial and we're gonna low degree extend it this way and then we're gonna lower extend in degree ",
        "16": "extended that way we're gonna load you agree extend it this way and degree extending this way in low degree extending this way is give us the same answer and so we have this square so then a light client to check data availability is gonna make basically 80 random queries into this structure but the light client is also going to download the merkel roots of every row ends the merkel roots of every column right so a light client has to download a square root of n data so in the case of like gigabytes this sum is gonna end up being about a few hundred kilobytes so a fraud proof instead of including all the entire data just has to prove fraud in one row or approve proof fraud in one column or prove inconsistency between one piss-up at one position and one row in one position in one column that's it right so because ultimately if the data is broken then it's going to be what that means is that some row is broken or ",
        "17": "some column is broken and so we just choose the specific row or column it's broken and you prove it so what this means is that the amount of data that a light client has to download goes up a bit because the light coin has to download these rubber boots and column routes but the size of our fraud proof goes all the way down to just proof proving fraud of one row right so now fraud proofs go down to being under a megabyte and we have a scheme that theoretically works there are impossible techniques for improving this well starks so this start with a scheme at the bottom this scheme at the bottom basically says we're gonna use Fri so we're going to use the the ingredient in Starks and what we're gonna do is we're gonna use this to basically give the same properties as the two-degree scheme but on top or so the two-dimensional ski but on top of simple one-dimensional codes right so it basically if you have a deep rhyme where that D Prime has a ",
        "18": "bug in it instead of providing the entire D Prime we're just gonna do a bit of clever math and we're going to basically use project use FRA here's one of these kind of f of X minus a minus y over big X minus X arguments that you might have heard about at all of this morning and we're gonna prove that there exists some output which is the correct value of extending the parts of D prime that you have to some position and look it doesn't match the thing that's in the hash stream so basically you can just use FRA to prove an inconsistency instead of providing ink it's proving an inconsistency by providing the entire data another thing you can do which is simpler conceptually and better but it's hard it takes more work and this may end up requiring a six to run effectively is to just directly use a Stark to verify correctness of the more covert right so ",
        "19": "basically what we're gonna prove is we're gonna just take all of the hashes in the Merkel tree line them up along a polynomial and it'll be a P evaluated at powers of Omega and P at Omega to the 2n all the way up to P at Omega to the 4n minus 1 like we're just gonna place the leaves there and then we're gonna do basically make it make an arguments to prove that these particular pieces of data represents a degree n or less than n polynomial so it means it's an actual low degree extension and then we're gonna just verify every other leaf and note in the Merkel tree by just showing P of X is the hash of P of x squared and P of x squared times W so every parent's as the hash of its two children you can shove this in a constraint directly because HS high degree need to break it up a bit and then you prove that you check that the root actually is art right so the nice thing about this holy grail approach is it basically says well now ",
        "20": "we don't even need fraud proofs and if you don't need fraud proofs then to check availability you actually we don't even need the network latency assumption all you need is basically this assumption that says well there is at least some minimal number of of honest clients somewhere in the network and that's it and that number is not 50 it's not 51 percent it's not five percent it's just a constant so two possible ways to improve to improve it so what do we use this for right so the first use case as I mentioned verifying existing chains so this is a fully kind of layer 2 technique that could be used to improve the security properties of existing light clients and basically what you do is you just connect your node you receive a piece of data provided by someone that claims to be a more amoral root of the chain and for anti-ddos reasons you could attach this to proof of work if there's a chain you've already authenticated you could ",
        "21": "also have this beer and have the provider provide I'm have a deposit in a smart contract so that deposit could be taken away if there is a fraud if there is a fraud proof and then on this root you perform the data availability challenges you wait for fraud proofs and if the data availability of challenges pass and you see no fraud proofs then you accept the chain is valid so you could theoretically come up with this scheme just make it be this way or two thing and just by having light clients do this you can improve the security of synching of existing block chains and by basically creating this mechanism that says well if there is a 51% attack and a light client sinks that syncs a chain then the the network basically will kind of collaborate over we detect the fraud past the fraud or from the fraud around and light clients by default are not going to accept this chain so it makes ",
        "22": "51 percent percent attacks kind of considerably less powerful use case to scalable chains right so basically in the theorem 2.0 it's a shorter chain and so there's way more data than most people can download individually and so you would just use fraud proofs and data availability proofs as this kind of additional check that you can download you you can basically get security guarantee is on data even if some whatever particular subcommittee was responsible for signing off on that data turns out to have been corrupted so that's you know no use case away or three assembly or two right so basically if you have some decks plasma chained thing and your decks plasma chained thing requires you users to verify data availability is so that they can make challenges which pretty much every plasma thing requires then you can use this to check for data availability instead of users downloading the data ",
        "23": "themselves or relying on some trusted Watchtower scheme yeah so conclusion data availability of verification and fraud proofs two nice ingredients you can combine them together you can of quickly verify large amounts of data and computation indirectly right so you can and if given these security assumptions you that are kind of much lighter than the security assumption of they're not being a 51% attack you can achieve this guarantee that says the data is available with a cryptographically high probability and that the chain is secure or every piece of data is valid because if there wasn't valid someone if it wasn't valid someone would have brought yes at a fraud proof and in all of these cases succinct zero knowledge proof so like starts yeah and make things better and in many cases reduce the need for one of the security assumptions the network latency one so that you don't need to worry about Network await and C ",
        "24": "at all and this is great thank you are there any questions should I go okay so you said that you can use this type of data availability proof for plasma and for watchtowers I think they disagree with this because in plasma if the availability if data has become unavailable and the state transition happens the damage has already been done so there's no advantage to you detecting no so the idea would be that whenever you receive a plus so in a kind of normal clients running plasma what you would do is you would shrine it download the data and if you fail to download the data then you would act it right so the scheme here would be a G when you see a new plasma ",
        "25": "block you would run at 8:00 availability check and if the data availability check fails instead of having download the full block to check that the data is always yeah so I'm not sure if it's a question but I would kept thinking through your presentation that when you say the data in terms of like data availability I assume that you mean the they block data with the transactions containing it because what makes me think about so there's an assumption that somebody could watch the chain or what you watch something and they can basically in a sub linear time figure out the fraud proofs but I would state that it's not even possible in the systems like aetherium or I would even say Bitcoin the problem is the state of course because if you're even if you know the ",
        "26": "that's the composition of the blocks and the transactions you would not be able to verify it unless you have the state which precedes this transaction and in order to compute the state you have to run everything from the beginning or do some other tricks like downloaded from the state first and that's why for example when people designed the fast sync in etherium they were thinking oh we're going to just fast sync to the snapshot and then probablistic ly verify everything that happened before but of course that didn't work because the state became so large that even probabilistic verification was impractical so what I'm saying is that the fraud proofs are the some something which probably will be very expensive to produce and I'm basically challenging the assumption that there will be a lot of people actually doing proofs at all so I would say this is the very weak assumption so in an aetherium context ",
        "27": "this scheme work working on top of a theory Macias does rely on yes basically at least one honest node having run an archive node which also has a bit of extra software attached if the QI given that we're looking at raising gas elimin sand then two more years happens and it goes down to or the case where there are zero people running archive nodes then like yes there is going to come a point where basically if forget historical data too much then they won't be able to create fraud proofs and they will end they won't even be able to create these morkul routes to verify data availability so yeah there is definitely a kind of implicit assumption that people keeps people keep storing data and if this is broken and then yes this scheme also fails one I mean if people do have the data then and but if you are an archive node then ",
        "28": "person producing the the fraud groups becomes easier also another thing that we could that you could do is you could have a scheme where basically for every block like basically a stateless client system where for every block you can generate the block plus the transactions plus the witness and you could potentially have like every node agree to like store 0.1% of witnesses or somewhere or something similar so there's two ways to go about this one is kind of what's a looking like the currency to approach which is basically a kind of establishing very strict States a state size control over the entire system and so like not really having much state so you can I do fraud proofs with just the the block and a tiny bit of extra data and the second approach is and if do a bit of stuff on top so basically it ensure that some nodes actually do have these witnesses ",
        "29": "in potential you could even just stick witnesses inside them inside of this data - thank you so much [Applause] you "
    }
}